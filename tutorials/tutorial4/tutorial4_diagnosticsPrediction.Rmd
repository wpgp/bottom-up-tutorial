---
title: "Population modelling \n for census support"
subtitle: "Tutorial 4: Advanced model diagnostics and prediction"
date: "Compiled on `r format(Sys.time(), '%d/%m/%Y')`"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
    theme: yeti
    number_sections: false
    css: [../css/index_custom.css, ../css/tutorial_custom.css]
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{r logo, echo=FALSE}
###LOAD WP LOGO##
htmltools::img(src = knitr::image_uri("./assets/pic/320px-UNFPA_logo.svg.png"),
               alt = 'logo', style = 'position:absolute; top:60px; right:0; padding:20px; width: 25%; height: auto')

```

![](../../assets/pic/wp_logotype_gray_low.png)

```{r, include=T, warning=F, message=F, class.source = 'fold-hide'}
# 1 Set-up ---

# load libraries
library(tidyverse) # managing data
library(ggdag) # drawing DAG
library(kableExtra) # visualising table
library(here) # handling path
library(rstan) # running Bayesian models
library(plotly) # interactive plot

# stan setup
options(mc.cores = parallel::detectCores()-1)
rstan::rstan_options(auto_write = TRUE) # speed up running time of compiled model
```

# Introduction

From Tutorial 1 to Tutorial 3, we built a **bottom-up population model**, 
modelling brick by modelling brick. In Tutorial 1 we modelled
population count as a **Poisson-Lognormal compound,** accounting for
settled area. In Tutorial 2, we added a **hierarchical random intercept** 
by settlement type and region, that differentiates parameter
estimation by the natural clustering of the observations. In Tutorial 3
we eventually modelled small-scale variations in population density by
adding **a linear regression based on geospatial covariates**. We
covered thus all the fundamental modelling blocks of WorldPop bottom-up
population model, described in @leasure2020a.

We will cover in this tutorial advanced model diagnostics for a complete
goodness-of-fit assessment. Once we check that the model successfully
passes the different diagnostics, we will then predict population count
for every grid cell of the study area. Predicting population will be the
opportunity to talk about estimates uncertainty.

### Goals

1.  Explore convergence issue
2.  Understand posterior predictive check
3.  Understand cross-validation and overfitting
4.  Predict population for every grid cell of the study area
5.  Visualise prediction uncertainty
6.  Aggregate prediction in custom spatial units

### Supporting readings

-   [Evaluating regression
    models](https://www.bayesrulesbook.com/chapter-10.html) by the
    Bayes rules! book, on posterior predictive check and the difference
    between correct estimation and correct modelling.

-   [Tidy data and Bayesian analysis make uncertainty visualization
    fun](https://www.mjskay.com/presentations/openvisconf2018-bayes-uncertainty-2.pdf)
    by Matthew Kay, University of Michigan. It hammers the importance of
    communicating uncertainty with estimates and explains how a Bayesian
    framework addresses that issue.

-   [You're fit and you know it: overfitting and
    cross-validation](https://medium.com/the-sound-of-ai/youre-fit-and-you-know-it-overfitting-and-cross-validation-90a3a9f67c74),
    by Andy Elmsley. Good introduction to cross-validation despite
    applying it to machine learning problems.

### Extra packages

We will use some additional packages, mainly for GIS manipulation:

-   `raster` for predicting the gridded population in a raster format

-   `sf` for aggregating the prediction in custom spatial unit

-   `tmap` to plot spatial data

-   `RColorBrewer` to use nice color palette in plots

```{r, eval=F}

install.packages('raster')
install.packages('RColorBrewer')
install.packages('sf')
install.packages('tmap')
```

Vignettes can be found here: [raster](https://rspatial.org/raster/1-introduction.html), 
[RColorBrewer](https://rdrr.io/cran/RColorBrewer/man/ColorBrewer.html), 
[sf](https://r-spatial.github.io/sf/articles/) and
[tmap](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html).

# Advanced model diagnostics

Before predicting population count for the entire study area we want to
make sure that the model is correct. We thus perform additional checks:
on the **model convergence** with traceplots and `stan` warnings, on the estimated parameters with a *posterior predictive check*
and on the predicted population count at study site with a
*cross-validation approach*.

We will work with the first relevant population model that we built in Tutorial 1, 
that is the "basic" Poisson-Lognormal model:

```{=tex}
\begin{equation}

population 〜 Poisson( pop\_density * settled\_area) \\

pop\_density 〜 Lognormal( \mu, \sigma) \\

\\

\mu 〜 Normal( 5, 4 ) \\

\sigma 〜 Uniform( 0, 4 )(\#eq:model2)

\end{equation}
```
We do some set-ups that should now be familiar.

```{r}
# load data
data <- readxl::read_excel(here('tutorials/data/nga_demo_data.xls'))
data <- data %>% 
  mutate(
    id = as.character(1:nrow(data)),
    pop_density=N/A
  )
# mcmc settings
chains <- 4
iter <- 500
seed <- 1789

# stan data
stan_data <- list(
  population = data$N,
  n = nrow(data),
  area = data$A)

# set parameters to monitor
pars <- c('mu','sigma', 'density_hat', 'population_hat')
```


## Assessing convergence

In this section we will explore models that have convergence issues and discover how to evaluate
where they stem from and how to fix it.

First let's look at the impact of the **warmup length**, the early phase of the simulations 
in which the sequences get closer to the mass of the distribution.

We choose a warmup of 20 iterations (instead of 250).

```{r}
warmup <- 20

# mcmc
fit_lowWarmup <- rstan::stan(file = file.path('tutorials/tutorial1/tutorial1_model2.stan'), 
                          data = stan_data,
                          iter = warmup + iter, 
                          chains = chains,
                          warmup = warmup, 
                          pars = pars,
                          seed = seed)
```
Stan shows several warnings. You can explore the meaning of them [here](http://mc-stan.org/misc/warnings.html).

Looking at the traceplot is, in this case, very informative:

```{r}
traceplot(fit_lowWarmup, c('mu', 'sigma'), inc_warmup=T)
```

The traceplots indicate that some exploration is still happening after the shaded area, 
that is after the end of the warmup period.We just need to increase the length of the  warmup period.

Next we will look at a more serious convergence issue.
Let's say that we choose **unfit priors** for our model.
Typically we can declare a Uniform with stricter bounds for $\sigma$:

```{=tex}
\begin{equation}

\sigma 〜 Uniform( 0, 1 )

\end{equation}
```

We store the model under `tutorial4/tutorial1_model2_wrong.stan` and estimate it.

```{r}
warmup <- 250

fit_wrong <- rstan::stan(file = file.path('tutorials/tutorial4/tutorial1_model2_wrong.stan'), 
                   data = stan_data,
                   iter = warmup + iter, 
                   chains = chains,
                   warmup = warmup, 
                   pars = pars,
                   seed = seed)
```

Many metrics detect a convergence issue. The most worrying one is the *divergent transitions* that are
often related with issues in the model structure itself and not only with estimation process.

```{r}
traceplot(fit_wrong, c('mu', 'sigma'))
```

The traceplot for $\mu$ indicates that the chains didn't mix. Moreover the traceplot for $\sigma$ 
shows clearly the issue in bounding with a ceiling at 1 that overly constrained the estimation.


## Checking predicted posterior
Let's come back to a correctly fit model.

```{r}
# mcmc
fit <- rstan::stan(file = file.path('tutorials/tutorial1/tutorial1_model2.stan'), 
                          data = stan_data,
                          iter = warmup + iter, 
                          chains = chains,
                          warmup = warmup, 
                          pars = pars,
                          seed = seed)
```


As shown in previous paragraph, Bayesian estimation highly relies on choosing correct 
priors for the parameters. Those
priors should reflect expert knowledge about the quantity of interest
that is, then, confronted with the observations. The prior model defines
the space for the posterior estimation. We should thus ensure that the
prior support did not overly constrain the posterior. This is called a
**posterior predictive check.**

We need first to extract the **estimated posterior distribution** from
the `stanfit` object. We do it using the `extract` function from
`rstan`.

```{r, }
mus <- data.frame(
    # extract parameter
    posterior=extract(fit, pars='mu')$mu,
    parameter = 'mu'
    )
glimpse(mus)
```

`mus` contains the estimated mu for each iteration post-warmup
(500) of each Markov chain (4).

We want to recreate the **prior distribution**. In model 2 of Tutorial
3, we used as prior: $$\mu \sim Normal(5,4)$$ We sample from this
prior distribution, the same number of draws that we have for the
posterior distribution (that is 2000).

```{r}
# retrieve stan parameter
post_warmup_draws <- iter - warmup

# simulate from the prior distribution
mus$prior <- rnorm(chains * post_warmup_draws, 5, 4)
```

We build for $\sigma$ a similar dataframe to `mus` that contrasts its prior
distribution with its posterior distribution and bind it
together with `mus`.

```{r posterior, fig.cap='Posterior predictive checks for alpha and sigma', class.source = 'fold-hide'}
# build dataframe with parameters distribution comparison
parameters <-  rbind(
  # alpha
  mus ,
  #sigma
  data.frame(
      posterior=extract(fit, pars='sigma')$sigma,
      prior = runif(chains * post_warmup_draws, 0, 4),
      parameter = 'sigma')
  )  %>% 
  pivot_longer(-parameter,
    names_to = 'distribution',
    values_to = 'value'
  )

# plot distribution comparison for both parameter
ggplotly(ggplot(parameters, aes(x=value, color=distribution, fill=distribution))+
  geom_density()+
  theme_minimal()+
    facet_wrap(.~parameter, scales = 'free'))
```

Figure \@ref(fig:posterior) shows a posterior predictive check for both
$\mu$ and $\sigma$. It compares the prior distribution that we set in
the model and the estimated posterior distribution. Don't hesitate to
zoom in, as the range of values is very different between the prior and
the posterior.

We confirm that our priors did not falsely constrain the posterior, with a very wide range of
possible values. The unique constrain we can observe is the lower bound
on $\sigma$ at 0 but this is normal as a variance has to be positive.

## Cross-validation of model predictions

After checking the model structure, we
revisit the **goodness-of-fit metrics based on population predictions**.

In tutorial 1 to 3, we check the goodness-of-fit of the modelled
predictions **in sample**, that is predicting population and comparing
it to the observations for the study sites that were already used to fit
the model.

**In-sample checks do not account for model overfitting.** Overfitting
occurs when the model does not only reproduce the information from the
observations but also the noise. We want the model to be able to
accurately predict population also unknown new study sites. This
real-world scenario can be reproduced by keeping a percentage of the
observations for model fitting and leaving the remaining study sites for
predicting, a mechanism called **cross-validation.**

We need first to split our data in two, a training set (70%) that will
be used for fitting the model and a predicting set (30%) that will be
used for model prediction:

```{r, message=F}
set.seed(2004)
# sample observations
train_size <-  round(nrow(data)*0.7)
train_idx <- sample(1:nrow(data), size = train_size, replace=F)

# build train datasets
train_data <- data[train_idx,]

# build test datasets
test_data <- data[-train_idx,]
```

In `stan`, cross-validation can be done simultaneously with model fitting
by providing the testing dataset to the `generated quantities` modelling
block. Because all variables have to be defined for each set, it lengthens
substantively the code. 

```{stan output.var="simpleCode", eval=F }
data{
  
  int<lower=0> n_train; // number of microcensus clusters in training set
  int<lower=0> n_test; // number of microcensus clusters in predicting set

  int<lower=0> population_train[n_train]; // count of people
  
  vector<lower=0>[n_train] area_train; // settled area in training
  vector<lower=0>[n_test] area_test; // settled area in testing
}

parameters{
  // population density
  vector<lower=0>[n_train] pop_density_train;
  
  //intercept
  real mu;

  // variance
  real<lower=0> sigma; 
}

model{
  
  // population totals
  population_train ~ poisson(pop_density_train .* area_train);
  pop_density_train ~ lognormal( mu, sigma );
  
  //  intercept
  mu ~ normal(5, 4);
  
  // variance
  sigma ~ uniform(0, 4);
}

generated quantities{
  
  int<lower=-0> population_hat[n_test];
  real<lower=0> density_hat[n_test];

  for(idx in 1:n_test){
    density_hat[idx] = lognormal_rng( mu, sigma );
    population_hat[idx] = poisson_rng(density_hat[idx] * area_test[idx]);
  }
}
```

We prepare the data for `stan` and run the model:

```{r}
# prepare data
stan_data_xval <- list(
  population_train = train_data$N,
  n_train = nrow(train_data),
  n_test = nrow(test_data),

  area_train = train_data$A,
  area_test = test_data$A,

  seed=seed
)

# mcmc setting
pars <- c('mu','sigma', 'population_hat',  'density_hat')

# mcmc
fit_xval <- rstan::stan(file = file.path('tutorials/tutorial4/tutorial1_model2xval.stan'), 
                   data = stan_data_xval,
                   iter = warmup + iter, 
                   chains = chains,
                   warmup = warmup, 
                   pars = pars,
                   seed = seed)
```

We now have access to two predictions sets: one, `fit`, from the
in-sample prediction and one, `fit_xval`, from the cross-validation
prediction.

We compare the goodness-of-metrics from the two models:

```{r xval, message=F, warning=F, class.source = 'fold-hide'}
# predict population
getPopPredictions <- function(model_fit, 
                              estimate='population_hat',
                              obs='N', reference_data=data){
  # extract predictions
  predicted_pop <- as_tibble(extract(model_fit, estimate)[[estimate]])
  colnames(predicted_pop) <- reference_data$id
  
  # summarise predictions
  predicted_pop <- predicted_pop %>% 
    pivot_longer(everything(),names_to = 'id', values_to = 'predicted') %>% 
    group_by(id) %>% 
    summarise(
      across(everything(), list(mean=~mean(.), 
                                upper=~quantile(., probs=0.975), 
                                lower=~quantile(., probs=0.025)))
      ) %>% 
    # merge with observations
    left_join(reference_data %>% 
                rename('reference'=all_of(obs)) %>% 
                select(id, reference), by = 'id') %>%
    # 
    mutate(
      residual= predicted_mean - reference,
      ci_size = predicted_upper- predicted_lower,
      estimate = estimate
      )
return(predicted_pop)
}

# build comparison dataframe between in-sample and xvalidation
comparison_df <- rbind(
 getPopPredictions(fit) %>% 
   mutate(Model='In-sample'),
 getPopPredictions(fit_xval, reference_data = test_data) %>% 
   mutate(Model='Cross-validation'))

# compute goodness-of-fit metrics
comparison_df %>% group_by(Model) %>% 
  summarise( `Bias`= mean(residual),
    `Inaccuracy` = mean(abs(residual)),
        `Imprecision` = sd(residual)
) %>%  kbl(caption = 'Goodness-of-metrics computed in-sample vs cross-validation') %>% kable_minimal()
```

Table \@ref(tab:xval) shows that all metrics shows a worse fit: the bias, the imprecision and the 
inaccuracy increase. However they still remains in the same range, which seems to indicate
that our model is not overfitting.

A good practice is to **always assess the model fit on the testing set** to avoid producing overly 
optimistic goodness-of-fit metrics.

# Gridded population prediction

We will cover in this section **model predictions for the entire study
area** at the grid cell level.

We will use our best population model, that is

```{r}
tutorial3_model2_fit <- readRDS("tutorials/tutorial3/tutorial3_model2_fit.rds")
```


It requires extracting the estimated parameters distributions:
$\hat\alpha_{t,r}, \hat\sigma, \hat\beta^{fixed}, \hat\beta^{random}_t$.
These estimated parameters are then combined with the input rasters
following this modified model for prediction:

```{=tex}
\begin{equation}

population\_predicted 〜 Poisson( \hat{pop\_density} * settled\_area) \\
\hat{pop\_density} 〜 Lognormal(\hat\mu, \: \hat\sigma) \\

\hat\mu = \hat\alpha_{t,r} + \hat\beta^{random}_t X^{random} + \hat\beta^{fixed} X^{fixed} (\#eq:predict)

\end{equation}
```
$X$ and $settled\_area$ are the input data at grid cell level:

Grid-cell based model prediction requires thus 10 input rasters:

-   settled area

-   settlement type

-   administrative region

-   the six covariates

-   a mastergrid that defines the grid cells for which to predict the
    population. These grid cells are (1) contained inside the study area, (2) considered as settled.

Unfortunately we are not allowed to redistribute the settlement map used in Nigeria v1.2, that
was provided by @oakridgenationallaboratory2018. We will thus provide
the code to show all the steps of population prediction but you will not
be able to run it.

We will focus on region 8 that contains only 671 110 cells instead of
the 166 412 498 cells for the entire country.

## Preparing data for prediction

To predict gridded population, we convert the rasters set to a table
with: in rows, the grid cells and in column, each raster value. We don't
need to keep the spatial structure of the raster for model prediction.
Every grid cell can be estimated independently as long as we know its
administrative region, settlement type and covariates attributes.

```{r, eval=F}
library(raster)
wd_path <- 'Z:/Projects/WP517763_GRID3/Working/NGA/ed/bottom-up-tutorial/'

# function that loads the raster and transforms it into an array
getRasterValues <- function(x){
  return( raster(paste0(wd_path,x))[])
}

# get a list of the rasters
rasterlist <- list.files(wd_path,
                         pattern='.tif', full.names = F)
# apply function to raster list
raster_df <- as_tibble(sapply(rasterlist, getRasterValues))
```

```{r, echo=F, eval=FALSE}
raster_df$gridcell_id <- 1:nrow(raster_df)

a <- raster_df %>% filter(mastergrid.tif==1) %>% dplyr::select(starts_with('settlement'), gridcell_id) 
sett_type <- 1:6
a <- a %>% 
    rowwise() %>%
    mutate(
      settlementType=sett_type[which.max(c_across(starts_with('settlement')))])
raster_df <- left_join(raster_df %>% dplyr::select(-starts_with('settlementarea')),
                       a %>% dplyr::select(gridcell_id, settlementType))

colnames(raster_df) <- str_remove(colnames(raster_df), '.tif')

raster_df <- raster_df %>% 
  mutate(mastergrid = ifelse(is.na(x1), 0, mastergrid),
         mastergrid = ifelse(settlementType==6, 0, mastergrid))

write_csv(raster_df %>% filter(mastergrid==1)%>% head(n=10), 'wd/raster_predict_ex.csv')
```

```{r, include=F, echo=F}
raster_df <- read.csv('wd/raster_predict_ex.csv')
```

The raster table contains 671 110 rows and 10 columns. 95% of the rows
are NAs because they are not considered as settled.

```{r}
settled <- raster_df %>% filter(mastergrid==1)
settled %>% head() %>% kbl() %>% kable_minimal()
```

We need to **scale the covariates** with the scaling factors computed
during the fitting stage.

```{r, eval=F}
#load scaling factor
scale_factor <- read_csv('tutorials/data/scale_factor.csv')

scaled_covs <- settled %>% 
  # subset covs column
  dplyr::select(starts_with('x'), gridcell_id) %>% 
  
  # convert table to long format
  pivot_longer(-gridcell_id, names_to='cov', values_to = 'value') %>%
  
  # add scaling factor
  left_join(scale_factor) %>% 
  
  # scale covs
  mutate(value_scaled= (value-cov_mean)/cov_sd) %>% 
  dplyr::select(gridcell_id, cov, value_scaled) %>% 
  
  # convert table back to wide format
  pivot_wider(names_from = cov, values_from = value_scaled)

# replace the covs with their scaled version
raster_df <- raster_df %>% dplyr::select(-starts_with('x')) %>% 
  left_join(scaled_covs)

```

## Extracting estimated parameters

The second step is to **extract the estimated parameter distributions**.
We use the model fitted with the full observations dataset and not the
one from the cross-validation for maximum information.

```{r, eval=F}
#extract betas
beta_fixed <- t(rstan::extract(model_fit, pars='beta_fixed')$beta_fixed)
beta_random <- t(rstan::extract(model_fit, pars='beta_random')$beta_random)
#extract alphas
alphas <- rstan::extract(model_fit, pars='alpha_t_r')$alpha_t_r
#extract sigmas
sigmas <-  rstan::extract(model_fit, pars='sigma')$sigma
```

We first focus on producing $\hat\beta^{fixed} X^{fixed}$ from Equation
\@ref(eq:predict), the covariates modelled with a fixed effect.

$\hat\beta^{fixed}$ is a `2000 x 5` matrix for the 5 covariates and 2000
estimations. We transposed it to multiply by $X^{fixed}$, a
`310 512 X 5` matrix of covariates for every settled cells.

```{r, eval=F}
# extract covariates modelled with a fixed effect
cov_fixed <- settled %>% 
  dplyr::select(x1:x3, x5:x6) %>% 
  as.matrix()

cov_fixed <- cov_fixed %*% beta_fixed
```

We then produce $\hat\beta^{random}_t X^{random}$, the covariates
modelled with a random effect.

$\hat\beta^{random}_t$ is a `2000 x 5` matrix for the 5 settlement
types. We need to associate each grid cell with the correct
$\hat\beta^{random}_t$ based on the grid cell settlement type and then
multiply by $X^{random}$ the covariates values.

```{r, eval=F}
beta_random <- as_tibble(beta_random)
# add settlement type to beta random
beta_random$settlementType <- 1:5

# extract covariates modelled with a random effect
cov_random <- settled %>% 
  # subset random covariate and settlement type
  dplyr::select(settlementType,x4) %>% 
  # associate correct estimated beta_t
  left_join(beta_random) %>% 
  # multiply cov by slope
  mutate(
    across(starts_with('V'), ~ .x*x4)
  ) %>% 
  # keep only the estimates
  dplyr::select(-settlementType, -x4) %>% 
  as.matrix()
```

We eventually need to associate the correct $\hat\alpha_{t,8}$ to each
grid cell.

$\hat\alpha_{t,8}$ has a similar format as $\hat\beta^{random}_t$. We
will thus proceed in the same way.

```{r, eval=F}
# subset alpha_t for region 8
alpha_t_8 <- as_tibble(t(alphas[,,8]))
# assign settlement type
alpha_t_8$settlementType <- 1:5

alpha_predicted <- settled %>% 
  dplyr::select(settlementType) %>% 
  left_join(alpha_t_8) %>% 
  dplyr::select(-settlementType) %>% 
  as.matrix()
```

We can finally compute $\hat\mu$ as the sum of $\hat\alpha_{t,r}$,
$\hat\beta^{random}_t X^{random}$ and $\hat\beta^{fixed} X^{fixed}$

```{r, eval=F}
# sum mu components
mu_predicted <- alpha_predicted + cov_fixed + cov_random
```

## Predicting population count for every grid cell

To estimate the grid-cell population, we need to simulate from Equation
\@ref(eq:predict) with the estimated $\hat\mu$ and $\hat\sigma$.

```{r, eval=F}
predictions <- as_tibble(mu_predicted) %>% 
  # add grid cell id and the settled area to mu
  mutate(
      gridcell_id= settled$gridcell_id,
      settled_area = settled$settled_area
    )  %>% 
  # long format
  pivot_longer(
    -c(gridcell_id, settled_area), 
    names_to = 'iterations', values_to = 'mu_predicted') %>%
  mutate(
    # add sigma iterations for every grid cell
    sigma=rep(sigmas, nrow(mu_predicted)),
    # draw density for a log normal
    density_predicted = rlnorm(n(), mu_predicted, sigma),
    # draw population count from a Poisson
    population_predicted = rpois(n(), density_predicted*settled_area)
    ) %>% 
  dplyr::select(-mu_predicted,-sigma, -density_predicted) %>% 
  # convert it back to grid cell x iterations matrix
  pivot_wider(-c(iteration, population_predicted),
      names_from = iteration,
      values_from = population_predicted
    ) 
```

`predictions` contains 2000 population predictions for every settled grid.
cell. Let's see how does it look like for the 5 settled grid cells:

```{r, echo=F}
predictions <-  readRDS('tutorials/data/gridded_pop_predicted.rds')
```

```{r}
predictions[1:5, ] %>% dplyr::select(gridcell_id, everything()) %>% 
  kbl() %>% kable_minimal()  %>% scroll_box(width = "100%")
```

Note: Model prediction is memory-intensive. We thus usually run it in
parallel for blocks of grid cells.

# Predicting distributions

Bayesian modelling is a stochastic modelling. It accounts thus for the
unknowns, caused by for example:

-   Weak predictors

-   Errors in input data

-   Incomplete sample representativity

-   Observations variation

These unknowns result in prediction uncertainty. More precisely it
results in predicting a distribution of the likely population count.

As seen in previous section, we have for every grid cell 2000
predictions. Figure \@ref(fig:ex-predict) shows an example for five
grids cells.

```{r ex-predict, class.source = 'fold-hide', fig.cap='Example of predicted population count for 5 grid cells and their mean population count'}
prediction_ex <- predictions %>% slice(1:5) %>% 
         pivot_longer(-gridcell_id, names_to = 'iterations', values_to = 'prediction') %>% 
  group_by(gridcell_id) %>% 
  mutate(
    mean_pop = paste0(gridcell_id, ' (mean=', round(mean(prediction)), ')')
  )


ggplotly(ggplot(prediction_ex,   aes(x=prediction, color=mean_pop)) +
  geom_density()+
  theme_minimal()+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  labs(x='Population predicted', color='\n\nGridcell id \n(mean population)')) %>%
  layout(margin=list(t=100))
```

A meaningful statistic to retrieve from the distribution is the mean,
which can be considered as the *most likely value.*

## Gridded population

The [gridded bottom-up population that WorldPop
released](https://wopr.worldpop.org/?/Population) are based on the mean
value per grid cell.

Let's see how we can create a gridded population from our predictions.

```{r, echo=F,message=F,warning=F}
library(raster)
raster_df <- readRDS('wd/gridcell_id.rds')
```

We first compute the mean prediction for every grid cell:

```{r}
mean_prediction <- tibble( 
  mean_prediction = apply(predictions %>% dplyr::select(-gridcell_id), 1, mean)
  )
```

We then add the unsettled grid cell by joining with the `raster_df`,
that contains every grid cell.

```{r, message=F}
mean_prediction <- mean_prediction %>% 
  # add gridcell_id
  mutate(gridcell_id = predictions$gridcell_id) %>% 
  # join unsettled grid cell
  right_join(raster_df %>% 
               dplyr::select(gridcell_id)) %>% 
  # sort according to position in the raster
  arrange(gridcell_id)
```

To retrieve the raster structure we load a reference master - typically
the mastergrid.

```{r}
wd_path <- 'Z:/Projects/WP517763_GRID3/Working/NGA/ed/bottom-up-tutorial/'
mastergrid <- raster(paste0(wd_path, 'mastergrid.tif'))
```

And assign the predicted population count.

```{r}
# create raster
mean_r <- mastergrid
# assign mean prediction 
mean_r[] <- mean_prediction$mean_prediction
```

We can finally plot the raster to see our gridded population:

```{r raster-mean, fig.cap='Gridded mean population prediction for region 8'}
library(RColorBrewer)
# create color ramp
cuts <- quantile(mean_prediction$mean_prediction,
                 p=seq(from=0, to=1, length.out=7), na.rm=T)
col <-  brewer.pal(n = 7, name = "YlOrRd")

#plot mean prediction
plot(mean_r, col=col)
```

Figure \@ref(fig:raster-mean) shows typical population spatial distribution. 
We see in the top-left corner a cluster of cities, with denser neighborhoods in the centers.
Lines of denser settlements are linked with the spatial distribution of population around roads.
In white are displayed the unsettled grid cells.

## Gridded uncertainty

We can also retrieve from the full posterior distribution, the 95%
credible interval (CI) for every prediction and the related relative
uncertainty computed as:

```{=tex}
\begin{equation}
uncertainty = \frac{upper\_CI - lower\_CI}{mean\_prediction}
\end{equation}
```
The relative uncertainty informs us about areas where the mean
prediction is less reliable.

To compute the gridded uncertainty, we follow the same process as for
the gridded mean prediction.

```{r, message=F}
# retrieve the 95% credible interval
ci_prediction <- apply(predictions %>% dplyr::select(-gridcell_id),1, quantile, p=c(0.025, 0.975))

ci_prediction <- as_tibble(t(ci_prediction)) %>% 
    # add gridcell_id
  mutate(gridcell_id = predictions$gridcell_id) %>% 
  # join unsettled grid cell
  right_join(raster_df %>% 
               dplyr::select(gridcell_id)) %>% 
  # sort according to position in the raster
  arrange(gridcell_id) %>% 
  mutate(
    mean_prediction = mean_prediction$mean_prediction,
    uncertainty = (`97.5%` - `2.5%`)/ mean_prediction
  )

# create uncertainty raster
uncertainty_r <- mastergrid
uncertainty_r[] <- ci_prediction$uncertainty

#plot uncertainty raster
cuts <- quantile(ci_prediction$uncertainty,
                 p=seq(from=0, to=1, length.out=7), na.rm=T)
col <-  brewer.pal(n = 7, name = "Blues")
plot(uncertainty_r, col=col)
```

We see that higher uncertainty can be observed on the outskirt of denser settlement.
This is often the case due to the rapid expansion of settlement
extent that is difficult to capture.

# Aggregating prediction

Gridded populations are great to visualise the spatial distribution of
population across an area. But often we want to get aggregates, that is
**a population estimate for a custom area** such as a city or the
catchment area of a hospital.

Let's look at an example. We manually drew the extent of one city in our
study area.

*Disclaimer*: the extents are not the official extent

```{r, message=F, fig.cap='Map of the city extent with the gridded population', warning=F}
library(sf)
library(tmap)
tmap_options(check.and.fix = TRUE)
city <- st_read(paste0(wd_path, 'study_city.gpkg'), quiet=T)

names(mean_r) <- 'Gridded population'
tmap_mode('view')
tm_shape(city)+
  tm_borders()+
  tm_shape(mean_r)+
  tm_raster()+
  tm_basemap('Esri.WorldGrayCanvas')
```

Getting the **mean population prediction for the city** corresponds to
computing zonal statistic on the gridded population, that is summing up
all the grid cells contained in the city.

```{r}
mean_city <- extract(mean_r, city, fun=sum, na.rm=TRUE, df=TRUE)
mean_city %>%  
  mutate(features = 'city', .after=ID) %>% 
  rename("Mean population prediction"=Gridded.population) %>% 
  kbl(caption='Mean population prediction for the city computed with the gridded population') %>% kable_minimal(full_width = F)
```

Getting the uncertainty for that estimate is more complicated. Indeed we
can't sum up uncertainty of the grid cell to retrieve the uncertainty of
the aggregate because credible intervals are based on quantile
computation, and quantiles can't be summed.

We need thus to reconstruct the full population prediction distribution
for the city by aggregating all grid-cells population prediction
distribution.

We first convert the city polygon to a raster with same extent as the
gridded population. This raster is made of 1s for grid cells inside the
city extent and 0s for grid cells outside the city.

```{r, message=F}
city_r <- rasterize(city, mean_r)
```

We convert the city raster to an array and join to `raster_df` to get
the grid cell id. We filter out the grid cells that belongs to the city
and then merge it with the predictions:

```{r, message=F}
city_prediction <- raster_df %>% 
  # select grid cell id from raster df
  dplyr::select(gridcell_id) %>% 
  # add city dummy
  mutate(city = city_r[]) %>% 
  # keep grid cells inside the city
  filter(city==1) %>% 
  # join predictions
  left_join(predictions) %>% 
  # keep predictions
  dplyr::select(starts_with('V'))

```

`city_prediction` contains all the grid cells comprised in the city with
the corresponding full population prediction distribution.

We first sum the predictions for every grid cells at each iteration.

```{r}
city_prediction <- as_tibble(apply(city_prediction,2, sum, na.rm=T))
```

`city_prediction` becomes an array of size 2000 that contains thus the
2000 population totals for the city.

We can then derive meaningful statistics from the distribution of
population total for the city.

```{r city-distrib, class.source = 'fold-hide', fig.cap='Full distribution of the predicted population total for the city'}
city_prediction_stats <- city_prediction %>% 
  summarise( 
    "Mean population" = round(mean(value)),
    "Upper bound" = round(quantile(value, p=0.975)),
    'Lower bound' = round(quantile(value, p=0.025)))

ggplot(city_prediction, aes(x=value))+
  geom_density(size=1, color='orange')+
  theme_minimal()+
    theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  labs(y='', x='Predicted population for the city')+
  annotate("segment",x=city_prediction_stats$`Mean population`,
           xend=city_prediction_stats$`Mean population`, y=0, yend=0.000005, size=1)+
  annotate('text',x=city_prediction_stats$`Mean population`+5000, y=0.0000012, hjust = 0, fontface =2,
           label=paste0('Mean prediction: \n', city_prediction_stats$`Mean population`, ' people'))+
  annotate("segment",x=city_prediction_stats$`Upper bound`,
           xend=city_prediction_stats$`Upper bound`, y=0, yend=0.000005)+
  annotate('text',x=city_prediction_stats$`Upper bound`+5000, y=0.000001, hjust = 0,
           label=paste0('97.5% prediction \nbound: \n', city_prediction_stats$`Upper bound`, ' people'))+
    annotate("segment",x=city_prediction_stats$`Lower bound`,
           xend=city_prediction_stats$`Lower bound`, y=0, yend=0.000005)+
    annotate('text',x=city_prediction_stats$`Lower bound`+5000, y=0.000001, hjust = 0,
           label=paste0('2.5% prediction \nbound: \n', city_prediction_stats$`Lower bound`, ' people'))
```

Figure \@ref(fig:city-distrib) shows that the mean prediction matches
the one computed with the gridded population raster. Furthermore we see
that our model produces predictions with a very wide credible interval.

Having the full distribution can answer questions such as *"what is the
probability that there is more than xx people in the area"* or *"with a
95% certainty, what is the maximum number of people in that area?"*


# Acknowledgements

This tutorial was written by Edith Darin from WorldPop, University of Southampton and 
Douglas Leasure from Leverhulme Centre for Demographic Science, University of Oxford, 
with supervision from Andrew Tatem, WorldPop, University of Southampton.
Funding for the work was provided by the United Nations Population Fund (UNFPA).

# License

You are free to redistribute this document under the terms of a Creative Commons Attribution-NoDerivatives 4.0
International ([CC BY-ND 4.0](https://creativecommons.org/licenses/by-nd/4.0/)) license.

# References
