---
title: "Population modelling \n for census support"
subtitle: "Tutorial 3: How to model small-scale spatial variations?"
date: "Compiled on `r format(Sys.time(), '%d/%m/%Y')`"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
    theme: yeti
    number_sections: false
    css: [../css/index_custom.css, ../css/tutorial_custom.css]
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{r logo, echo=FALSE, purl=F}
###LOAD WP LOGO##
htmltools::img(src = knitr::image_uri("../../assets/pic/320px-UNFPA_logo.svg.png"),
               alt = 'logo', style = 'position:absolute; top:60px; right:0; padding:20px; width: 25%; height: auto')

local({
  hook_output <- knitr::knit_hooks$get('warning')
  knitr::knit_hooks$set(warning = function(x, options) {
    if (!is.null(options$max.height)) options$attr.warning <- c(
      options$attr.warning,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```
```{r, echo=F, fig.show="hold", out.width="20%", warning=F, message=F,}
library(here)
knitr::include_graphics(here('./assets/pic/wp_logotype_grey_hi.png'))
knitr::include_graphics(here('./assets/pic/USH0149_LOGO-2021_RGB_Neutral_Punched-AW.svg'))
knitr::include_graphics('../../assets/pic/Ox_LCDS_logo_bw.png')
```
```{r, include=T, warning=F, message=F, class.source = 'fold-hide'}
# 1 Set-up ----

# load libraries
library(tidyverse) # managing data
library(ggdag) # drawing DAG
library(kableExtra) # visualising table
library(here) # handling path
library(rstan) # running Bayesian models
library(plotly) # interactive plot

# stan setup
options(mc.cores = parallel::detectCores()-1)
rstan::rstan_options(auto_write = TRUE) # speed up running time of compiled model
```

# Introduction

Tutorial 2 explored how to model large-scale spatial patterns:
how population density differs per large grouping such as by region or
settlement type. We integrated those large-scale variations in a
Bayesian framework by using a hierarchical random intercept model for the population 
density.

Tutorial 3 aims at integrating small-scale variations of population
density that are linked to local context of human settlement. 
Figure
\@ref(fig:raster) shows how high-resolution geospatial covariates provide precise information
on local context.

```{r raster, echo = FALSE, purl=F, fig.cap = "Example of a covariate (residential area) around Onitscha, Nigeria", fig.align='center'}
# raster example
knitr::include_graphics(here("./assets/pic/tuto3_rasterZoom.png"))
```

Adding high-resolution covariates helps to improve the model fit as well
as guides population prediction in unsampled areas.

For data to be used as covariates in the model, they should be:

-   correlated with differences in population density

-   measured consistently and completely across the study space

-   accurately mapped as high-resolution geospatial layers

High-resolution covariates that are suitable for the modelling are
typically **spatial covariates with national coverage and consistent
data collection**. While individual or household-level information, such
as collected during surveys, is useful for understanding differences in
demographic characteristics, that type of information is difficult to
use in the bottom-up approaches because it only comes from a sample of
households. The primary objective of the population model is to make a
spatially-complete prediction.

### Goals

1.  Understand requirements for covariates to be included in the model
2.  Get an overview of covariates used in WorldPop bottom-up population
    models
3.  Familiarize with covariates processing for population model
4.  Integrate a linear regression to refine the definition of the median
    of the Lognormal
5.  Add a random slope model
6.  Introduce the concept of initialisation in a MCMC model estimation

### Supporting readings

To include a spatial covariate in the modelling and to use it as a
support for prediction, we use gridded datasets, known as *rasters*. You
would need thus to have basic GIS knowledge about raster and vector file
management. It does not need to be in `R`, it can be in QGIS, ArcGIS,
Python or any GIS software of your choice.

The purpose of this tutorial is **not** spatial data processing. We will
just mention processing techniques that are required to prepare
covariates data for population modelling.

However, here are some R resources on spatial manipulation:

-   The [bible](https://keen-swartz-3146c4.netlify.app/) by Pebesma and
    Bivand on the `sf` (vector data) and `stars` (raster data) R
    packages with excellent overview of the different spatial
    manipulations

-   A [hands-on introduction](https://geoscripting-wur.github.io/) to
    `sf` and `raster` package by the University of Wageningen

-   A [focus on raster manipulation](https://rspatial.org/) with
    `raster` and `terra` by Hijmans

# Formal modelling

We will work from model 3 in tutorial 2 which is based on a Poisson
distribution to model population count and on a Lognormal distribution
with a hierarchical random intercept by settlement type and region to
model population density .

Because the median of the population density is only defined with a
random intercept, it results in 5 (number of settlement type) x 11
(number of region) options for the population density estimates. To add
small-scale variations we **refine the median of the Lognormal with a
regression model** that integrates the covariates.

More formally, let's define $X$ a matrix of size *number of
observations* x *number of covariates* that contains the covariates
values for each study site and $\beta$ a vector of size the number of
covariates. Based on Equation 3 in tutorial 2 (and removing the prior
distribution for $\alpha_{t,r}$ for sake of readability), we define
$\mu$ the median of the Lognormal distribution as follows:

```{=tex}
\begin{equation}

population 〜 Poisson( pop\_density * settled\_area) \\
pop\_density 〜 Lognormal(\mu, \: \sigma) \\

\mu = \alpha_{t,r} +  X \beta \\[10pt]

\beta 〜 Normal(0,10)(\#eq:model1)


\end{equation}
```
Note that the prior for $\beta$ are identical normal distribution for
each covariate with mean 0 and standard deviation 10 to avoid
introducing any bias.

Figure \@ref(fig:dag1) shows the updated dependent relationships of our
model when integrating covariates.

```{r dag1, echo=F, purl=F, fig.cap='Dependancy graph for model 1 of tutorial 3'}
dagify(
    Y ~ alpha_1+alpha_2+ alpha_3+sigma + beta+ X,
    alpha_1 ~ alpha,
    alpha_2 ~ alpha,
    alpha_3 ~ alpha,
    outcome = 'Y',
    latent = 'alpha'
  ) %>%
    tidy_dagitty(seed=7) %>% 
    mutate(color=c('data','parameter','parameter','parameter','parameter','parameter','parameter','parameter','parameter','data')) %>% 
    ggplot(aes(x = x, y = y, xend = xend, yend = yend, color=color,shape=color)) +
    geom_dag_point() +
    geom_dag_edges() +
    geom_dag_text(col = "grey20",size=4,  parse=T) +
    scale_shape_manual(values=c(15,19))+
    theme_dag()+ labs(title = '', color='', shape='')
```

# Review of the covariates used in WorldPop

To date, five bottom-up population models have been produced at
WorldPop:

-   WorldPop. 2019. Bottom-up gridded population estimates for
    **Nigeria, version 1.2**.\
    WorldPop, University of Southampton.
    <https://dx.doi.org/10.5258/SOTON/WP00655>.

-   WorldPop (School of Geography and Environmental Science, University
    of\
    Southampton). 2020. Bottom-up gridded population estimates for
    **Zambia, version 1.0**.\
    <https://dx.doi.org/10.5258/SOTON/WP00662>

-   WorldPop and Institut National de la Statistique et de la
    Démographie du Burkina Faso. 2021. Census-based gridded population
    estimates for **Burkina Faso (2019), version 1.0**. WorldPop,
    University of Southampton.
    <https://dx.doi.org/10.5258/SOTON/WP00687>.

-   Boo G, Darin E, Leasure DR, Dooley CA, Chamberlain HR, Lazar AN,
    Tatem AJ. 2020.\
    Modelled gridded population estimates for the Kinshasa,
    Kongo-Central, Kwango, Kwilu,\
    and Mai-Ndombe provinces in the **Democratic Republic of the Congo,
    version 2.0**.\
    WorldPop, University of Southampton.
    <https://dx.doi.org/10.5258/SOTON/WP00669>

-   WorldPop. 2020. Bottom-up gridded population estimates for the
    Kinshasa, Kongo-Central, Kwango, Kwilu, and Mai-Ndombe provinces in
    the **Democratic Republic of the Congo, version 1.0**.  WorldPop,
    University of Southampton.
    <https://dx.doi.org/10.5258/SOTON/WP00658>

In addition to that, two models are currently being updated: **Nigeria
version 2.0** and **Democratic Republic of the Congo v3.0**.

Those six models encompass a large range of covariates, that are
specific to each country. Table \@ref(tab:review) offers an overview of
the final covariates set selected for each model.

```{r review, purl=F, message=F, warning=F, class.source = 'fold-hide'}
review_cov <- read_csv(here('./tutorials/tutorial3/covs_review.csv'))

review_cov %>% arrange(Type) %>% kbl(caption='Review of covariates used in WorldPop bottom-up population models') %>% kable_minimal()
```

The covariates selected can be broadly classified as describing four
main drivers of local population density variation:

1.  *previous population spatial distribution* through WorldPop top-down
    population disaggregation [@worldpopresearchgroup2018]
2.  *infrastructure* through OpenStreetMap
    [@openstreetmapcontributors2018], country-specific resources (for
    example @institutgeographiqueduburkinafaso2015) or modelled
    resources (for example travel time to cities [@weiss2018])
3.  *natural features* through remote sensing product such as radar data
    (Sentinel-1) or dry matter productivity (Copernicus)
4.  *settlement shape* through the morphology of building footprints
    [@ecopia.ai2019] or settled area [@oakridgenationallaboratory2018]

Other covariates sources that were considered (but not selected in the
final models) were:

-   Conflict locations from the [Armed Conflict Location and Event Data
    Project](https://acleddata.com/data-export-tool/)

-   Climatic variables from the [Climatic Research Unit at the
    university of
    Anglia](https://catalogue.ceda.ac.uk/uuid/10d3e3640f004c578403419aac167d82)

-   Active mining concessions from the [IPIS
    group](https://ipisresearch.be/publication/mining-concessions-dr-congo/)

-   Land cover classification from the [European Spatial
    Agency](https://climate.esa.int/en/esa-climate/esa-cci/)

-   Global forest change from the [University of
    Maryland](https://climate.esa.int/en/esa-climate/esa-cci/)

-   Elevation and slope from
    [WorldDEM](https://www.airbus.com/newsroom/press-releases/en/2018/10/WorldDEM-now-entirely-edited-and-available-via-streaming.html)

-   Fossil fuel emissions from the [Open-source Data Inventory for
    Anthropogenic CO2 Project](https://www.odiac.org/index.html)

To build a model, we first gather all covariates that can be related to
our specific population data. It can reach up to 900+. We then use
geospatial analysis techniques to obtain **gridded version of the
covariates with identical spatial resolution, alignment and extent**. It
involves resampling and clipping for covariates provided as raster files
or for covariates provided as vector files computing count, density,
distance to nearest features or even interpolation techniques.

# Covariates engineering

Further covariates engineering steps can help extracting even more
information from the gathered covariates.

1.  Log-transformation

Considering the logarithm of covariates helps handling extreme
values.

2.  Focal statistics

Focal statistics consist in summarising covariates in a moving window
around each grid cell. As seen in Table \@ref(tab:review), we used
different window sizes (1km, 2km or 5km) and summary statistics (mean or
coefficient of variation). It provides contextual information around the
grid cells.

3.  Standardisation

Scaling the covariate (that is subtracting the mean and dividing by
the standard deviation) helps enhancing meaningful local variations
from the mean. The scaling can even be refined by computing the mean and
the standard deviation by region, such that local variations are
representative of the region.

#### A note on covariate selection

After engineering the gathered covariates, we might end up with 1000+
potential covariates.

To select the best one for prediction purposes, we generally used one of the two
following methods:

-   pairwise correlation and scatter plot with the population density at
    study site level

-   univariate model, testing each covariate successively

# Including covariates in the model

We focus in the remaining parts of the tutorial on the data we
downloaded from @leasure2020 which corresponds to the Nigeria model
v1.2.

## Overview of the covariates in Nigeria, v1.2

Six covariates were used in Nigeria v1.2 model:

-   `x_1`: gridded population estimates from WorldPop Global
-   `x_2`: school densities within a 1km radius
-   `x_3`: household sizes by interpolating Demographic Health Survey
    results from 2013
-   `x_4`: settled area within a 1km radius
-   `x_5`: residential area in a 1km radius
-   `x_6`: nonresidential settled area within a 1km radius

Covariate `x_4` was scaled based on its mean and standard deviation
nationally, whereas covariates `x_5` and `x_6` were scaled based on
their mean and standard deviation within a 50-km radius. Leasure et al
scaled `x_5` and `x_6` in this way because they suspected that
neighborhood types may not be directly comparable across regions
(especially northern versus southern Nigeria). This scaling also reduced
correlation with `x_4` .

They scaled the WorldPop Global estimates (`x_1`) based on their mean
and standard deviation nationally. They treated this covariate
as an indicator of relative population densities based on the geospatial
covariates that were used in the random forest model.

Covariate `x_2` was scaled using its mean and standard deviation within
a 50km radius. They scaled this covariate within a 50km moving window
because what constitutes a "high density" of schools varies by region
and this distinction was lost when the covariate was scaled nationally.
This also helped to control for possible differences in school mapping
effort in different regions.

They scaled `x_3` based on its mean and standard deviation nationally.
One key reason for including this covariate was to account for a strong
north--south gradient in household sizes, with significantly more people
per household in northern Nigeria than in southern Nigeria.

## Preparing the data

To integrate the covariates in the model, we build first a dataset with
the average of the covariate values for each study site using zonal
statistics.

*Note that this constitutes a change in support: we might want to check
if the range of covariates values at study site level is representative
of the covariate values at grid cell level.*

Figure \@ref(fig:scatter) shows the relation between the covariates and
the population density at study site level. We see that household size
(`x_3`) is positively associated with population density. The
negative value are due to the scaling method adopted. On the opposite,
nonresidential settled area (`x_6`) is negatively associated with population density
which is
expected: the more the surroundings are nonresidential the lower the
population density.

```{r scatter, message=F, fig.cap='Scatterplot of covariates values vs population density for each study site', class.source = 'fold-hide'}
# 2. Covariates preparation ----
# load data
data <- readxl::read_excel(here('tutorials/data/nga_demo_data.xls'))
data <- data %>% 
  mutate(
    pop_density=N/A,
    id = as.character(1:nrow(data))
  )

# contrast covariates with pop density
data_long <- data %>% 
  pivot_longer(starts_with('x'), names_to = 'cov')

ggplot(data_long, aes(x=pop_density,y=value))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE,color='orange')+
  theme_minimal()+
  facet_wrap(.~cov, ncol=3, scales = 'free')+
  labs(x='Population density', y='')

```

Before implementing the model in `stan`, we **uniformly scale the
covariates at study site level**, such that the $\beta_k$ have the same
scale. We first compute the scaling coefficients (mean and standard
deviation) for each covariate:

```{r, message=F}
# compute scaling factors (mean and sd)
covariatesScaling <- function(var){
  mean_var <- mean(var)
  sd_var <- sd(var)
  return(
    data.frame(
      'cov_mean'= mean_var,
      'cov_sd' = sd_var
    )
  )
} 

covs <- data %>% 
  select(starts_with('x'))

scale_factor <- bind_rows(apply(covs, 2, covariatesScaling))
scale_factor$cov <- colnames(covs)

scale_factor %>% select(cov, cov_mean, cov_sd) %>% kbl %>%  kable_minimal()
```

We then apply the scaling coefficient to the covariates:

```{r}
# apply scaling factors to covariates
covs_scaled <-  covs %>% 
  mutate(cluster_id = 1:nrow(covs)) %>% 
  pivot_longer(-cluster_id,names_to = 'cov') %>% 
  left_join(scale_factor, by="cov") %>% 
  mutate(value= (value-cov_mean)/cov_sd ) %>% 
  select(-cov_mean, -cov_sd) %>% 
  pivot_wider(names_from = cov, values_from = value, id_cols = cluster_id) %>% 
  select(-cluster_id)

```

We store the scaled covariates and the scaling coefficients for the
prediction stage (in Tutorial 4).

```{r}
# save scaling factor
write_csv(covs_scaled, here('tutorials/data/covs_scaled.csv'))
write_csv(scale_factor, here('tutorials/data/scale_factor.csv'))
```

## Implementing the model

Equation \@ref(eq:model1) is implemented in `stan` as follows:

```{stan output.var="simpleCode", eval=F,  purl=F }
// Model 1: Hierarchical alpha by settlement type , region + covariates
data{
  ...
  // slope
  int<lower=0> ncov; // number of covariates
  matrix[n, ncov] cov; // covariates
}
parameters{
  ...
  // slope
  row_vector[ncov] beta; 
}
transformed parameters{
  ...
  for(idx in 1:n){
    pop_density_median[idx] = alpha_t_r[type[idx], region[idx]] + sum( cov[idx,] .* beta );
  }
}
model{
  ...
  //slope
  beta ~ normal(0,10);
}
generated quantities{
  ...
   for(idx in 1:n){
    density_hat[idx] = lognormal_rng( alpha_t_r[type[idx], region[idx]] + sum(cov[idx,] .* beta), sigma );
   }
}
```

Note the two new data types - `matrix` for the covariate values and
`row_vector` for $\beta$ - as well as the new operator, `.*` . 
A row vector is a matrix with one row. `.*` performs elementwise multiplication.
We need these elements because of the way we coded the
linear regression for `pop_density_median`:in a `for` loop running through each study site
defined by their `idx`. We extract for each study site 
the corresponding covariates values, `cov[idx,]` which is a row vector. 
To get a vector of each covariate value associated with each parameter $\beta_k$, `beta` needs to
be a `row_vector` and the multiplication needs to be performed element by element.

We keep the same set up for the MCMC:

```{r}
# 3. Modelling with covariates ----

# mcmc settings
chains <- 4
warmup <- 500
iter <- 500
seed <- 1789
```

And we add the covariates to `stan` input data:

```{r}
# prepare data for stan
stan_data_model1 <- list(
  population = data$N,
  n = nrow(data),
  area = data$A,
  type = data$type,
  ntype= n_distinct(data$type),
  region = data$region,
  nregion = n_distinct(data$region),
  seed=seed,
  cov = covs_scaled,
  ncov = ncol(covs_scaled)
  )
```

We add `beta` as parameter to monitor and run the model.

```{r,max.height='150px'}
pars <- c('alpha','sigma','beta','alpha_t', 'nu_alpha', 'nu_alpha_t', 'population_hat',  'density_hat')

# mcmc
fit1 <- rstan::stan(file = file.path(here('./tutorials/tutorial3/tutorial3_model1.stan')), 
                   data = stan_data_model1,
                   iter = warmup + iter, 
                   chains = chains,
                   warmup = warmup, 
                   pars = pars,
                   seed = seed)
```

The model runs into convergence issues.
The error message that is returned is not very informative.
However the `Viewer` pane in Rstudio contains more information, namely:

`Chain 2: Exception: lognormal_lpdf: Scale parameter is 0, but must be > 0!`.

It means that the combination of $\alpha$, $\beta$ and covariates value
that is currently being tested lead to some zeros in the scale term of the lognormal
which is forbidden.


### A note on initialisation

This is an opportunity to discuss about **initialisation**. MCMC
simulations start exploring the parameter space from one initial value.

This initial value is controlled in `stan` by the option `init`. Its default is
`random`, that is "*Let Stan generate random initial values for all
parameters. The seed of the random number generator used by Stan can be
specified via the seed argument. If the seed for Stan is fixed, the same
initial values are used. The default is to randomly generate initial
values between -2 and 2 on the unconstrained support".*

Defining the starting values helps the algorithm to start close to the region of interest
such that no time is lost in exploring an area of the parameter space that we know 
doesn't fit the likely values and might run into parameters combination that is unfit to our model structure.
Note that the initialisation is not constraining, 
it just gives a hint to the algorithm. 

We initialise only *root parameters*. Root parameters are parameters that don't
depend on other parameters. Dependent parameters will then inherit the initialisation.
```{r}
# add initialisation
inits.out <- list()
set.seed(stan_data_model1$seed)

for (c in 1:chains){
  inits.i <- list()

  inits.i$sigma <- runif(1, 0.4, 0.8)
  inits.i$alpha <- runif(1, 3, 6)
  inits.i$beta <- runif(stan_data_model1$ncov, -1, 1)
  
  inits.out[[c]] <- inits.i
}
```
Note that we define initial values for each chain. We base them around estimated values in previous models,  
and add some random jittering.

We run the estimation with these initialisation values.
```{r, max.height='150px'}
fit1bis <- rstan::stan(file = file.path(here('./tutorials/tutorial3/tutorial3_model1.stan')), 
                   data = stan_data_model1,
                   iter = warmup + iter, 
                   chains = chains,
                   warmup = warmup, 
                   pars = pars,
                   seed = seed,
                   init= inits.out)
```
The former error message is not displayed anymore and the model contains samples.
We still observe the same issue of tutorial 2, the integer overflow for the prediction of population count in the warmup period.
We can safely say that the model has converged.

We can plot the $\hat\beta_k$. The sign and
magnitude of the covariate effects are inline with the association shown
in Figure \@ref(fig:scatter).

```{r, message=F}
# plot beta estimation
stan_plot(fit1, pars='beta', fill_color='orange')
```

## Comparing prediction with previous model

The next question is: **How much improvement is brought by integrating covariates in the model?**

We load Tutorial 2 model 3 to compare:

```{r, max.height='150px'}
# load tutorial 2 final model
fit0 <- readRDS(here('./tutorials/tutorial2/tutorial2_model3_fit.rds'))
```

We can then compute the predictions for every study site and compare the
goodness-of-fit when adding the covariates.

```{r, message=F, class.source = 'fold-hide', warning=F}
# extract predictions
getPopPredictions <- function(model_fit, 
                              estimate='population_hat',
                              obs='N', reference_data=data){
  # extract predictions
  predicted_pop <- as_tibble(extract(model_fit, estimate)[[estimate]])
  colnames(predicted_pop) <- reference_data$id
  
  # summarise predictions
  predicted_pop <- predicted_pop %>% 
    pivot_longer(everything(),names_to = 'id', values_to = 'predicted') %>% 
    group_by(id) %>% 
    summarise(
      across(everything(), list(mean=~mean(.), 
                                upper=~quantile(., probs=0.975), 
                                lower=~quantile(., probs=0.025)))
      ) %>% 
    left_join(reference_data %>% 
                rename('reference'=all_of(obs)) %>% 
                select(id, reference), by = 'id')%>% 
    mutate(
      residual= predicted_mean - reference,
      ci_size = predicted_upper- predicted_lower,
      estimate = estimate
      )

return(predicted_pop)
}

comparison_df <- rbind(
 getPopPredictions(fit0) %>% 
   mutate(Model='Without covariates'),
 getPopPredictions(fit1) %>% 
   mutate(Model='With covariates'))

# compute goodness-of-fit metrics
comparison_df %>% group_by(Model) %>% 
  summarise( `Bias`= mean(residual),
    `Inaccuracy` = mean(abs(residual)),
        `Imprecision` = sd(residual)
) %>%  kbl(caption = 'Goodness-of-metrics comparison with and without covariates ') %>% kable_minimal()
```

We see an improvement on every goodness-of-fit metrics.

# Grouping and covariates effect: a random slope model

In tutorial 2 we have seen that a model fitting a unique $\alpha$ for
all the observations could be improved by splitting the observations
into groupings that would share a similar pattern of population density.

The idea is similar with $\beta$: **some covariates effects might vary
by grouing**. Example: `x_4`, the sum of settled area within a
1km radius might, might have a greater predictive power in rural areas
than in urban areas. The difference relationships between covariate and
population density by settlement type is highlighted in Figure
\@ref(fig:scatter-t).

```{r scatter-t, class.source = 'fold-hide', fig.cap='Scatterplot of covariates vs population density by settlement type', message=F}
# 4. Modelling covariates with random slope ----

ggplot(data_long %>% 
         group_by(type) %>% 
         mutate(
           type = paste0(type,' n=' ,n()),
           type=as.factor(type)), aes(x=pop_density,y=value, color=type))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  theme_minimal()+
  facet_wrap(.~cov, ncol=3, scales = 'free')+
  labs(y='', x='Population density', color='Settlement type')

```

Modelling $\beta_k$ by settlement type is called a **random slope
model**.

::: {.question}
**Question**: Do we want to model the $\beta_k$ hierarchically?
:::

<details><summary>Click for the solution</summary>

Modelliong $\beta$ hierarchically means assuming that there is a national pattern
with subnational refinement. The $\beta_{k,t}$ can however have opposite 
directions (see Figure \@ref(fig:scatter-t)) 
which speaks against a common overarching $\beta$.

</details>

Formally, a random slope model is written as follows:

```{=tex}
\begin{equation}

population 〜 Poisson( pop\_density * settled\_area) \\
pop\_density 〜 Lognormal(\mu, \: \sigma) \\[10pt]

\mu = \alpha_{t,r} + \beta^{random}_t X^{random} + \beta^{fixed} X^{fixed} \\[15pt]

\beta^{random}_t 〜 Normal(0,10) \\
\beta^{fixed} 〜 Normal(0,10)

\end{equation}
```
The difference can be seen in the indexing: $\beta^{random}$ is indexed
by $t$. Similarly as in the no-pooling framework in Tutorial 2, we set the priors to
be independent prior, namely Normal(0,10).

The `stan` implementation is as follows:

```{stan output.var="simpleCode", eval=F, purl=F }
// Model 1: Hierarchical alpha by settlement type , region + covariates
data{
  ...
    // fixed slope
  int<lower=0> ncov_fixed; // number of covariates -1
  matrix[n, ncov_fixed] cov_fixed; // covariates
  // random slope
  vector[n] cov_random;
}
parameters{
  ...
  // slope
  row_vector[ncov_fixed] beta_fixed; 
  vector[ntype] beta_random;
}
transformed parameters{
  ...
  vector[n] beta;

  for(idx in 1:n){
    beta[idx] = sum( cov_fixed[idx,] .* beta_fixed) + cov_random[idx] * beta_random[type[idx]];
    pop_density_median[idx] = alpha_t_r[type[idx], region[idx]] + beta[idx];
  }
}
model{
  ...
  //slope
  beta_fixed ~ normal(0,10);
  beta_random ~ normal(0,10);
}
generated quantities{
  ...
 vector[n] beta_hat;

  for(idx in 1:n){
    beta_hat[idx] = sum( cov_fixed[idx,] .* beta_fixed) + cov_random[idx] * beta_random[type[idx]];
    density_hat[idx] = lognormal_rng( alpha_t_r[type[idx], region[idx]] + beta_hat[idx], sigma );
  ...
}
```

Note that we wrote the code to model only **one** random covariate, such that `beta_random`
is only a vector that contains the $\beta^{random}_t$. For implementing several
random effect we would need a matrix (*settlement type x number of covariates*).

To run the model, we distinguish in the input data between the
covariates that are fixed and the one that is random. We choose `x_4` ,
the sum of settled area within a 1km radius, to be modelled with a
random effect.

Note that this setting allows to test the model with different covariate
candidates for the random effect.

```{r,  max.height='150px'}
# prepare stan data
stan_data_model2 <- list(
  population = data$N,
  n = nrow(data),
  area = data$A,
  type = data$type,
  ntype= n_distinct(data$type),
  region = data$region,
  nregion = n_distinct(data$region),
  seed=seed,
  cov_fixed = covs_scaled %>% select(-x4),
  ncov_fixed = ncol(covs_scaled) -1,
  cov_random = covs_scaled$x4
  )

pars <- c('alpha','sigma','beta_fixed','beta_random','alpha_t','alpha_t_r', 'nu_alpha', 'nu_alpha_t', 'population_hat',  'density_hat')

# initialise
inits.out <- list()
set.seed(stan_data_model2$seed)

for (c in 1:chains){
  inits.i <- list()
  # intercept
  inits.i$sigma <- runif(1, 0.4, 0.8)
  inits.i$alpha <- runif(1, 3, 6)
  inits.i$beta_fixed <- runif(stan_data_model2$ncov_fixed, -1, 1)
  inits.i$beta_random <- runif(stan_data_model2$ntype, -1, 1)

  inits.out[[c]] <- inits.i
}

# mcmc
fit2 <- rstan::stan(file = file.path(here('./tutorials/tutorial3/tutorial3_model2.stan')), 
                   data = stan_data_model2,
                   iter = warmup + iter, 
                   chains = chains,
                   warmup = warmup, 
                   pars = pars,
                   seed = seed,
                   init = inits.out)
```

No convergence issue. We plot `beta_random` that is a vector with a
$\hat\beta_t$ for each settlement type.

```{r, message=F}
# plot beta estimation
stan_plot(fit2, pars='beta_random', fill_color='orange')+
    # add alpha from tutorial 1
  geom_vline(xintercept=-0.006515444, size=1.5, linetype=2)+
  annotate('text', x=0.1, y=5.7, label="beta for cov 4 \nfrom first model")
```

We see that modelling $\beta^{x4}$ by settlement type unravels different
patterns: we observe a non-significant effect for settlement 1 and 3 the most urbanised
types. This was expected as the sum of settled area is likely to be homogeneous across urbanised area.
We see also that the previous estimtaed $\beta_4$ was masking effect in opposite direction
between settlement 2 and settlement 4,5.

We now want to evaluate the effect on the predicted population count for
each study site.

```{r}
# extract predictions
comparison_df <- rbind(
 getPopPredictions(fit1) %>% 
   mutate(model='Fixed effect'),
 getPopPredictions(fit2) %>% 
   mutate(model='Random effect in x4'))
# compute goodness-of-fit metrics
comparison_df %>% group_by(model) %>% 
  summarise( `Bias`= mean(residual),
    `Inaccuracy` = mean(abs(residual)),
        `Imprecision` = sd(residual)
) %>%  kbl(caption = 'Goodness-of-metrics comparison with and without random effect in x4 ') %>% kable_minimal()
```

We see a slight decrease of bias and an increase of the precision of the
estimates.

We will save the results of this final model as a RDS file to explore it
in Tutorial 4.

```{r, eval=T}
# save model
saveRDS(fit2, here('./tutorials/tutorial3/tutorial3_model2_fit.rds'))
```

# Acknowledgements

This tutorial was written by Edith Darin from WorldPop, University of Southampton and 
Douglas Leasure from Leverhulme Centre for Demographic Science, University of Oxford, 
with supervision from Andrew Tatem, WorldPop, University of Southampton.
Funding for the work was provided by the United Nations Population Fund (UNFPA).

# License

You are free to redistribute this document under the terms of a Creative Commons Attribution-NoDerivatives 4.0
International ([CC BY-ND 4.0](https://creativecommons.org/licenses/by-nd/4.0/)) license.

# References
